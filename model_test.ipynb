{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0deb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "# !pip install transformers\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7474ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8da5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcacfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b964144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "    images = []\n",
    "    for image_path in image_paths:\n",
    "        i_image = Image.open(image_path)\n",
    "        if i_image.mode != \"RGB\":\n",
    "            i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "        images.append(i_image)\n",
    "\n",
    "    pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "\n",
    "    output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with sample image\n",
    "# need to upload image to google colab first\n",
    "image_paths = [\"/content/family.jpg\"]\n",
    "img = Image.open(\"/content/family.jpeg\")\n",
    "display(img)\n",
    "predict_step(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926ef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model from Salesforce\n",
    "# Model page: https://huggingface.co/Salesforce/blip-image-captioning-baseß\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import requests\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f5db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# >>> a photography of a woman and her dog\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# >>> a woman sitting on the beach with her dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoModelForVision2Seq, AutoTokenizer, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28892a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "qwen_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    qwen_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id)\n",
    "qwen_processor = AutoProcessor.from_pretrained(qwen_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = \"\"\"\n",
    "You are a compassionate storyteller. Using the attached photo, craft a 120–180 word micro‑story intended for an older adult and their family. The story should gently evoke memories, spark warm conversation, and support emotional well‑being.\n",
    "\n",
    "Guidelines:\n",
    "- Focus on mood, place, season, and relationships; avoid listing objects.\n",
    "- Weave in 2–3 sensory details (sounds, scents, textures, light).\n",
    "- Use warm, respectful language and short, vivid sentences.\n",
    "- Avoid definitive claims about names, ages, or locations. Use gentle, tentative phrasing (perhaps, it seems, maybe).\n",
    "- If people appear, emphasize connection and small rituals rather than appearance.\n",
    "- Be inclusive and avoid stereotypes; balance nostalgia with quiet hope.\n",
    "- If text is clearly legible in the image, you may thoughtfully incorporate it.\n",
    "- If the scene is ambiguous, lean into universal themes (family, gatherings, journeys, everyday moments).\n",
    "\n",
    "Output:\n",
    "- 1–2 paragraphs of story.\n",
    "\n",
    "Variants (pick one voice if you want to steer style):\n",
    "- Voice A (third‑person close): Tell the story from a gentle narrator’s view.\n",
    "- Voice B (first‑person elder): Write as if an older adult is recalling the moment in the photo.\n",
    "- Voice C (second person): Address a loved one directly, with tenderness and gratitude.\n",
    "\n",
    "Examples of style knobs you can add:\n",
    "- Tone: warm and hopeful; lightly bittersweet; playful nostalgia.\n",
    "- Era cues: hint at a decade only if strongly suggested by the image.\n",
    "- Cultural touch: include respectful, non‑stereotyped details only if clearly present.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "short_prompt = \"\"\"Write a 120–180 word micro‑story inspired by this photo for an older adult and their family. Describe the photo honestly, write the story to evoke gentle reminiscence and well‑being. Use warm, simple language, 2–3 sensory details, and avoid object lists. Use tentative phrasing for uncertain facts. Emphasize connection and small rituals.\"\"\"\n",
    "\n",
    "# Both long and short prompt works well. Use long one to generate story with variants (different perspective), use short one to generate clean and short story.\n",
    "# I also tried two outputs of generating 1. 1–2 paragraphs of story 2. Then add “Conversation starters:” followed by two open‑ended, gentle questions that invite sharing (e.g., “What songs did you hear at gatherings like this?”).\n",
    "# However, generating two open-ended questions that invite sharing (to echo with our proposal of sparking meaningful family conversation) did not work well (sometimes it is off topic or too general) so I removed it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story_qwen(image_path, prompt=short_prompt):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": prompt}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    text_prompt = qwen_processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = qwen_processor(text=[text_prompt], images=[image], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output_ids = qwen_model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "    generated_text = qwen_tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return generated_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d674bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/content/family.jpeg\" # should later replace with input\n",
    "# three examples for testing [\"img_1.jpg\", \"img_2.jpg\", \"img_3.png\"]\n",
    "image_story = generate_story_qwen(image_path)\n",
    "### usually takes about 2 mins to generate the result on T4 GPU\n",
    "\n",
    "print(image_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac662f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output using short prompt and image_path = \"/content/family.jpeg\"\n",
    "# The sun was warm on their backs as they sat together on the soft sand, \n",
    "# the ocean's gentle lapping behind them. \n",
    "# The sky was a clear blue, and the air was filled with the salty scent of the sea. \n",
    "# They were a family, four of them, all smiling at the camera. \n",
    "# The older adult, perhaps their grandmother, had her arm around her daughter, who was holding a young boy close. \n",
    "# The boy, with his bright eyes and curious smile, looked up at his parents, who were also beaming. \n",
    "# It felt like a perfect day, a moment frozen in time, a reminder of simpler times when life was less rushed and more about enjoying each other’s company.\n",
    "# The sound of laughter mingled with the waves, creating a symphony of happiness that filled the air.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
